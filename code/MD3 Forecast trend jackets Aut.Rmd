---
title: "Dissertation RMarkdown N.3- Topic: Forecasting retrieving data from Google Trends
  downloaded directly and manualy from Google Trends website."
author: "Abel Hernandez Garcia"
date: "05/08/2023"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Objective

The objective of this R Markdown is to do forecasts with data from Google Trends, this data is going to be retrieved from the Google API, not from a .cvs file. 
Note: I wrote this R code based on Christopher Yee's work: https://www.christopheryee.org/blog/mining-google-trends-data-with-r-featuring-gtrendsr/


# Section 1

## Calling packages needed. 
First we need to load the packages that we are going to need. 

#### Introducing `prophet`
`prophet`is a package developed by Facebook to automatically forecast data "based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects" (Google, 2023).

```{r message=FALSE} 
library(readr)
library(dplyr)
library(ggplot2)
library(gtrendsR)
library(prophet)
library(lubridate)

```

## Retrieving data
**First we retrieve the data from Google Trends**
With the gtrendsR package, we can retrieve the data and specify the keywords, location and time. 

```{r}

device <- gtrends(
  keyword = c("laptop","desktop","tablet"),
  geo = "GB",
  time = "today+5-y",
  tz =0
)
```


Now we convert the dates in the data to the correct format, also we remove noisy data from the last period.

```{r} 

device_timeseries <- as_tibble(device$interest_over_time) %>% 
  mutate(date = ymd(date)) %>% # CONVERT DATE FORMAT
  filter(date < Sys.Date() - 1) # REMOVE "NOISY" DATA FROM LAST MONTH


```

## Plot data
Here we plot the data for the 3 keywords, we customize the plot, adding a title, subtitle and more settings.

```{r}

device_timeseries %>% 
  ggplot() +
  geom_line(aes(date, hits, color = keyword), size = 0.5) +
  scale_y_continuous(limits = c(0, 100)) +
  scale_color_brewer(palette = 'Set1')+
  theme_minimal() +
  labs(x = NULL,
       y = "Relative Search Interest",
       color = NULL,
       title = 'Google Trends: interest over time (Great Britain)',
       caption = "Extracted from Google Trends for this dissertation")

```
Here we can  observe the trends for the 3 keywords.


# Section 2

## Generating the pronostic for the word *laptop*.


### Preparing the dataframe 
In this part we convert columns to ds for date, and y for the values, this format is needed for the package `prophete` that we are going to use for forecasts. Also, the data is filtered to work just with the keyword **laptop**.
```{r}
laptop_timeseries <- device_timeseries %>% 
  filter(keyword == 'laptop') %>% 
  select(date, hits) %>% 
  mutate(date = ymd(date)) %>% 
  rename(ds = date, y = hits) %>% 
  arrange(ds) #ARRANGE BY DATE
```

### Building the model
Here we use the prophet tool and predict 2 years (730 days) from today (not from the last data record).

```{r}
laptop_m <- prophet(laptop_timeseries)
laptop_future <- make_future_dataframe(laptop_m, periods = 730, freq = "day", include_history = TRUE) # PREDICT 730 DAYS
laptop_ftdata <- as_tibble(predict(laptop_m, laptop_future))
```

## Two ways to plot the data.
#### There are 2 ways to plot the data, one is totally manually and the other one is automatically by the `prophet` package itself.

### Ploting the data manually. 

#### Combine forecast with actuals
First, it is needed to combine the forecast with the actual data.

```{r}
laptop_forecast <- laptop_ftdata %>% 
  mutate(ds = ymd(ds),
         segment = case_when(ds > Sys.Date()-1 ~ 'forecast',
                             TRUE ~ 'actual'), # SEGMENT ACTUAL VS FORECAST DATA
         keyword = paste0("laptop")) %>% 
  select(ds, segment, yhat_lower, yhat, yhat_upper, keyword) %>% 
  left_join(laptop_timeseries) # JOIN ACTUAL DATA

```
#### Plot forecasting results
Here we plot the data showing the forecast generated by prophet. 

```{r}
laptop_forecast %>% 
  rename(date = ds,
         actual = y) %>% 
  ggplot() +
  geom_line(aes(date, actual)) + # PLOT ACTUALS DATA
  geom_point(data = subset(laptop_forecast, segment == 'forecast'),
             aes(ds, yhat), color = 'green', size = 0.1) + # PLOT PREDICTION DATA
  geom_ribbon(data = subset(laptop_forecast, segment == 'forecast'),
              aes(ds, ymin = yhat_lower, ymax = yhat_upper), 
              fill = 'green', alpha = 0.3) + # SHADE PREDICTION DATA REGION
  scale_y_continuous(limits = c(0,100)) +
  theme_bw() +
  labs(x = NULL, y = "Relative Search Interest",
       title = "Google Trends: interest over time for \"laptop\" (Worldwide)")

```
### Ploting the data with the `prophet` plotting tool.

For this, we do not need to code for combine the forecast with the actual data, nor code/configure to plot it . The `dyplot.prophet` function makes all the work

```{r}
dyplot.prophet(laptop_m , laptop_ftdata , uncertainty = TRUE)

prophet_plot_components(laptop_m , laptop_ftdata , uncertainty = TRUE)
```


## Summary and Conclusions

The experimentation with the `prophet` package and different settings reveals interesting insights about forecasting relative search interest for the "laptop" keyword. The default settings provide a baseline forecast, while adjusting seasonality and adding holidays can lead to different predictions. The choice of settings depends on the specific characteristics of the data and the forecasting requirements.

Further exploration and tuning of the model settings, such as growth, changepoints, and prior scales, can be performed to optimize the forecast accuracy. Additionally, including other relevant features or external factors may enhance the model's performance in capturing underlying patterns.

Forecasting is an iterative process, and continuous monitoring and evaluation of the model's performance against actual data are essential to improve the forecast over time.

Overall, `prophet` is a powerful tool for forecasting time series data, and its flexibility allows users to tailor the model to their specific needs and make accurate predictions for various applications.






# Section 3

## Analysing  granular data 
Google provides weekly data when we request trends from the last 5 years, but what if we want to have the data from the last years but daily? For that purpose we use the `trendecon` package.

#### Installing the package. 

We need to install the package `remotes` once is installed, we proceed installing `trendecon`:
```{r message=FALSE}

remotes::install_github("trendecon/trendecon")

library(trendecon)

```

### Usage

#### To download series from Google Trends with the `trendecon` package:

Description of **ts_gtrends** function from the `trendecon` package and the difference with the **gtrends** function from the `gtrendsR` package: ts_gtrends function is a wrapper around gtrendsR::gtrends() that modifies the original function by a) vectorizing it, b) converting the result to tsboxable tibble and c) retries if no result is returned.


```{r}
x <- 
  ts_gtrends(
    keyword = "laptop",
    category = "0",
    geo = "GB",
    time = "today+5-y",
    retry = 5, #Number of attempts, in case the query request does not succeed.
    wait = 5, #conds to wait between attempts, where waiting time is attempt * wait.
)

tsbox::ts_plot(x) #Plotting the data

```

### Retrieving granular data
Now we are going make a robust timeseries. Google provides weekly data when we request trends from the last 5 years, but what if we want to have the data from the last years but daily? For that reason we use the function **ts_gtrends_mwd** from the `trendecon` package.

```{r message=FALSE} 
robust_laptop <- ts_gtrends_mwd(
  keyword = "laptop",
  category = "0",
  geo = "GB",
  from = "2019-01-01", #The starting date.
  to = Sys.Date() #the ending date. Sys.Date() is for the current day.
)
```

### Generating the pronostic

#### Converting the data to the right format for `prophet`.
As I have made in the second section, we convert columns to ds for date, and y for the values.

```{r}
robust_laptop_timeseries <- robust_laptop %>% 
  select(time, value) %>% 
  mutate(time = ymd(time)) %>% 
  rename(ds = time, y = value) %>% 
  arrange(ds) #ARRANGE BY DATE

```

### Building the model
Here we use the prophet tool and predict 2 years (730 days).

```{r}
r_laptop_m <- prophet(robust_laptop_timeseries)
r_laptop_future <- make_future_dataframe(r_laptop_m, periods = 730, freq = "day", include_history = TRUE) # PREDICT 730 DAYS
r_laptop_ftdata <- as_tibble(predict(r_laptop_m, r_laptop_future))

```


### Plot forecasting results
Here the  data is plotted to show the forecast generated by prophet. 


```{r}

dyplot.prophet(r_laptop_m , r_laptop_ftdata , uncertainty = TRUE)

prophet_plot_components(r_laptop_m , r_laptop_ftdata , uncertainty = TRUE)

```


Here we can see that the data is daily, but since there are big differences between the results from the weekdays and the weekends days we can see that the standard deviation in this forecast is much bigger. 


# Section 4

## Using different pronostic models.

In order to use different types of models, we can use the package `forecast`. In this section I will use the **ARIMA** and **ETS** models.


#### Load necessary libraries
```{r}
library(tsbox)
library(forecast)
```


#### Plot the Google Trends data
```{r}
tsbox::ts_plot(robust_laptop_timeseries)
```

### Forecast using exponential smoothing state space model ETS and ARIMA models


```{r}
laptop_forecast_ets <- forecast(ets(robust_laptop_timeseries$y), h = 100) #ETS
 
laptop_forecast_arima <- forecast(auto.arima(robust_laptop_timeseries$y), h = 100) #ARIMA
```

### Plot the forecast

```{r}
plot(laptop_forecast_ets, main = "ETS Forecast")
plot(laptop_forecast_arima, main = "ARIMA Forecast")
```
Here we can see the plots for the ARIMA and ETS models. 

